{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005c50df-c4c1-405e-93b5-10175356de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter_contrib_nbextensions\n",
    "# !jupyter contrib nbextension install - user\n",
    "# from jedi import settings\n",
    "# settings.case_insensitive_completion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67443260-a0bc-4605-b53c-be64ae15d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install ai platform and kfp\n",
    "# USER_FLAG = \"--user\"\n",
    "# !pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
    "# !pip3 install {USER_FLAG} kfp --upgrade\n",
    "# !pip install google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88044735-902b-41ef-8f4c-2e01821eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d591c5b2-2d78-4d10-9363-b4a3aff21951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud services enable compute.googleapis.com         \\\n",
    "#                        containerregistry.googleapis.com  \\\n",
    "#                        aiplatform.googleapis.com  \\\n",
    "#                        cloudbuild.googleapis.com \\\n",
    "#                        cloudfunctions.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19aaf51-d63d-4771-92f0-45c356bc586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ab6367-523a-4b42-a5da-123405dc2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\"\n",
    "#!gcloud auth login if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af458713-51b7-465f-be5d-e76d5c2d1de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpa-poc-001'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412eb35a-3148-4af2-97e1-a31e1b7064b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://gpa-churn/artifacts/pipeline-vertexai/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://gpa-churn/artifacts\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline-vertexai/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fe1b1b-fc92-442a-81cb-797b0c3adab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'southamerica-east1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGION=\"southamerica-east1\"\n",
    "REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc46021-ba2b-4fcd-8d49-f1af7ba62244",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement_list = [\n",
    "    \"pandas==1.3.5\",\n",
    "    \"scikit-learn\",\n",
    "    \"pickle-mixin\",\n",
    "    \"numpy\",\n",
    "    \"jupyterlab==3.1.12\",\n",
    "    \"ipywidgets>=7.6\",\n",
    "    \"matplotlib==3.3.4\",\n",
    "    \"jupyter-dash\",\n",
    "    \"plotly==5.3.1\",\n",
    "    \"pytest==6.2.2\",\n",
    "    \"seaborn==0.11.1\",\n",
    "    \"glob2==0.7\",\n",
    "    \"SQLAlchemy==1.3.24\",\n",
    "    \"lightgbm==3.2.0\",\n",
    "    \"tabulate==0.8.9\",\n",
    "    \"shap==0.39.0\",\n",
    "    \"optuna==2.6.0\",\n",
    "    \"dython==0.6.4\",\n",
    "    \"minepy==1.2.5\",\n",
    "    \"pyarrow==3.0.0\",\n",
    "    \"kmodes==0.11.0\",\n",
    "    \"dash==1.19.0\",\n",
    "    \"dash-daq==0.5.0\",\n",
    "    \"nltk\",\n",
    "    \"unidecode\",\n",
    "    \"fsspec\",\n",
    "    \"gcsfs\",\n",
    "    \"joblib\",\n",
    "    \"great-expectations==0.13.17\",\n",
    "    \"google-cloud-storage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20534a88-4c71-4959-a3d3-ebd434803ebd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ff9151-db05-4b51-a99a-c05a0f6fb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    #packages_to_install=requirement_list,\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src@sha256:fdc6b2fed2deac0ea267878cfde028f3b30079b104a6f4047d120c820def1b83\",\n",
    "    output_component_file=\"get_preprocessed_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_preprocessed_data(\n",
    "    Xtrain_: Output[Dataset],\n",
    "    Xval_: Output[Dataset],\n",
    "    ytrain_: Output[Dataset],\n",
    "    yval_: Output[Dataset],\n",
    "    prefix:str='gs://gpa-churn/data/processed/input/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import gc\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    bucket = prefix.split('/')[2]\n",
    "    storage_client = storage.Client()\n",
    "    obj_list = storage_client.list_blobs(bucket)\n",
    "    obj_list = [i.name for i in obj_list if 'data/processed/input/' in i.name]\n",
    "    obj_list = obj_list[1:]\n",
    "    df_list = []\n",
    "    for obj in obj_list:\n",
    "        local_df = pd.read_parquet('gs://gpa-churn/'+obj)\n",
    "        df_list.append(local_df)\n",
    "        print(f'added {prefix}{obj}')\n",
    "\n",
    "    df = pd.concat(df_list, axis=0)\n",
    "    df.drop(columns=['cod_cliente'], inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    target = 'target'\n",
    "    features = list(df.columns)\n",
    "    features = [i for i in features if i != target]\n",
    "\n",
    "    Xtrain, Xval, ytrain, yval = train_test_split(\n",
    "        df[features], \n",
    "        df[[target]],\n",
    "        test_size=0.15, \n",
    "        random_state=501\n",
    "        )\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    Xtrain.reset_index(drop=True, inplace=True)\n",
    "    Xval.reset_index(drop=True, inplace=True)\n",
    "    ytrain.reset_index(drop=True, inplace=True)\n",
    "    yval.reset_index(drop=True, inplace=True)\n",
    "    print('Successfully read training data.')\n",
    "    print('shapes:')\n",
    "    print(f'xtrain:{Xtrain.shape}, ytrain:{ytrain.shape}')\n",
    "    print(f'xval:{Xval.shape}, yval:{yval.shape}')\n",
    "    \n",
    "    Xtrain.to_parquet(Xtrain_.path + '.parquet', index=False, compression='gzip')\n",
    "    Xval.to_parquet(Xval_.path + '.parquet', index=False, compression='gzip')\n",
    "    ytrain.to_parquet(ytrain_.path + '.parquet', index=False, compression='gzip')\n",
    "    yval.to_parquet(yval_.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3260a28e-d24f-44c0-a77d-68f3cab71b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    #packages_to_install=requirement_list,\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src@sha256:fdc6b2fed2deac0ea267878cfde028f3b30079b104a6f4047d120c820def1b83\",\n",
    "    output_component_file=\"feature_engineering_sequence.yaml\"\n",
    ")\n",
    "\n",
    "def feature_engineering_sequence(\n",
    "    Xtrain_: Input[Dataset],\n",
    "    Xval_: Input[Dataset],\n",
    "    Xtrain_fe: Output[Dataset],\n",
    "    Xval_fe: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    Xtrain = pd.read_parquet(Xtrain_.path + \".parquet\")\n",
    "    Xval = pd.read_parquet(Xval_.path + \".parquet\")\n",
    "    \n",
    "    numerical_columns = [\n",
    "        'val_venda_bruta_cupom',\n",
    "        'qtd_item_venda',\n",
    "        'flg_vend_meu_desct',\n",
    "        'valor_desconto',\n",
    "        'flag_dev',\n",
    "        'tipo_promo_0',\n",
    "        'tipo_promo_1',\n",
    "        'tipo_promo_2',\n",
    "        'tipo_promo_3',\n",
    "        'tipo_promo_4',\n",
    "        'tipo_promo_5',\n",
    "        'categoria_0',\n",
    "        'categoria_1',\n",
    "        'categoria_2',\n",
    "        'categoria_3',\n",
    "        'categoria_4',\n",
    "        'categoria_5',\n",
    "        'categoria_6',\n",
    "        'categoria_7',\n",
    "        'departamento_0',\n",
    "        'compras_mes',\n",
    "        'agg_l3m_val_venda_bruta_cupom',\n",
    "        'agg_l3m_qtd_item_venda',\n",
    "        'agg_l3m_flg_vend_meu_desct',\n",
    "        'agg_l3m_valor_desconto',\n",
    "        'agg_l3m_flag_dev',\n",
    "        'agg_l3m_tipo_promo_0',\n",
    "        'agg_l3m_tipo_promo_1',\n",
    "        'agg_l3m_tipo_promo_2',\n",
    "        'agg_l3m_tipo_promo_3',\n",
    "        'agg_l3m_tipo_promo_4',\n",
    "        'agg_l3m_tipo_promo_5',\n",
    "        'agg_l3m_categoria_0',\n",
    "        'agg_l3m_categoria_1',\n",
    "        'agg_l3m_categoria_2',\n",
    "        'agg_l3m_categoria_3',\n",
    "        'agg_l3m_categoria_4',\n",
    "        'agg_l3m_categoria_5',\n",
    "        'agg_l3m_categoria_6',\n",
    "        'agg_l3m_categoria_7',\n",
    "        'agg_l3m_departamento_0',\n",
    "        'agg_l3m_compras_mes',\n",
    "    ]\n",
    "\n",
    "    outlier_columns_mean = [\n",
    "        'pib_percapita',\n",
    "        'idade',\n",
    "        'delta_de_cadastro',\n",
    "        'delta_de_stix'\n",
    "    ]\n",
    "\n",
    "    yeojohnson_columns = [\n",
    "        'val_venda_bruta_cupom',\n",
    "        'qtd_item_venda',\n",
    "        'flg_vend_meu_desct',\n",
    "        'valor_desconto',\n",
    "        'compras_mes',\n",
    "        'agg_l3m_val_venda_bruta_cupom',\n",
    "        'agg_l3m_qtd_item_venda',\n",
    "        'agg_l3m_flg_vend_meu_desct',\n",
    "        'agg_l3m_valor_desconto',\n",
    "        'agg_l3m_compras_mes',\n",
    "        'pib_percapita',\n",
    "        'idade',\n",
    "        'delta_de_cadastro'\n",
    "    ]\n",
    "    \n",
    "    # training set\n",
    "    #-------------------------------------------------------\n",
    "    fe_pipeline = Pipeline([\n",
    "        ('drop_temporary_columns', pipeline_modules.drop_temporary_columns()),\n",
    "        ('drop_with_low_variance', pipeline_modules.drop_numerical_with_variance(columns=numerical_columns)),\n",
    "        ('encode_sex_column', pipeline_modules.encode_sex_column()),\n",
    "        ('group_rare_regions', pipeline_modules.group_rare_categorical(columns=['region'], threshold=0.002)),\n",
    "        ('encode_regions', pipeline_modules.encode_categorical(columns=['region'])),\n",
    "        ('handle_outliers_max', pipeline_modules.outlier_handling(\n",
    "            columns=numerical_columns, \n",
    "            method='gauss', \n",
    "            band=2.8, \n",
    "            action='max')),\n",
    "        ('handle_outliers_mean', pipeline_modules.outlier_handling(\n",
    "            columns=outlier_columns_mean, \n",
    "            method='gauss', \n",
    "            band=2.5, \n",
    "            action='mean')),\n",
    "        ('handle_negative_values', pipeline_modules.handle_negative_values(columns=numerical_columns)),\n",
    "        ('fill_missing_numerical_zero', pipeline_modules.fill_na_values_with_zero(\n",
    "            columns=['ind_email','cadastro_stix','delta_de_cadastro','delta_de_stix'])),\n",
    "        ('fill_missing_numerical_mean', pipeline_modules.fill_na_values_with_zero(\n",
    "            columns=['pib_percapita','idade'])),\n",
    "        ('transform_yeojohnson', pipeline_modules.data_transformation(\n",
    "            columns=yeojohnson_columns, \n",
    "            method='yeojohnson'))\n",
    "    ])\n",
    "\n",
    "    Xtrain = fe_pipeline.fit_transform(Xtrain)\n",
    "\n",
    "    # validation set\n",
    "    #-------------------------------------------------------\n",
    "    Xval = fe_pipeline.transform(Xval)\n",
    "    \n",
    "    # save feature engineering artifacts\n",
    "    #-------------------------------------------------------\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name='gpa-churn'\n",
    "    model_file='artifacts/training_pipeline/fe_pipeline/fe_pipeline.joblib'\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(model_file)\n",
    "    joblib.dump(fe_pipeline, 'fe_pipeline.joblib')\n",
    "    blob.upload_from_filename('fe_pipeline.joblib')\n",
    "    \n",
    "    Xtrain.to_parquet(Xtrain_fe.path + '.parquet', index=False, compression='gzip')\n",
    "    Xval.to_parquet(Xval_fe.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e41c6a4-0b4b-43da-9b8a-5be87b0bb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    #packages_to_install=requirement_list,\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src@sha256:fdc6b2fed2deac0ea267878cfde028f3b30079b104a6f4047d120c820def1b83\",\n",
    "    output_component_file=\"feature_selection_sequence.yaml\"\n",
    ")\n",
    "\n",
    "def feature_selection_sequence(\n",
    "    Xtrain_fe: Input[Dataset],\n",
    "    Xval_fe: Input[Dataset],\n",
    "    ytrain_: Input[Dataset],\n",
    "    yval_: Input[Dataset],\n",
    "    Xtrain_fs: Output[Dataset],\n",
    "    Xval_fs: Output[Dataset],\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.utils as utils\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    Xtrain = pd.read_parquet(Xtrain_fe.path + \".parquet\")\n",
    "    Xval = pd.read_parquet(Xval_fe.path + \".parquet\")\n",
    "    ytrain = pd.read_parquet(ytrain_.path + \".parquet\")\n",
    "    yval = pd.read_parquet(yval_.path + \".parquet\")\n",
    "    \n",
    "    # training set\n",
    "    #-------------------------------------------------------\n",
    "    fs_pipeline = Pipeline([\n",
    "            ('select_with_correlation', pipeline_modules.select_with_correlation(\n",
    "                threshold=0.82, \n",
    "                method='recursive',\n",
    "                objective='classification'))\n",
    "        ])\n",
    "        \n",
    "    Xtrain = fs_pipeline.fit_transform(Xtrain, ytrain)\n",
    "    \n",
    "    # validation set\n",
    "    #-------------------------------------------------------\n",
    "    Xval = fs_pipeline.transform(Xval)\n",
    "    \n",
    "    # save feature selection artifacts\n",
    "    #-------------------------------------------------------\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name='gpa-churn'\n",
    "    model_file=f'artifacts/training_pipeline/fs_pipeline/fs_pipeline.joblib'\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(model_file)\n",
    "    joblib.dump(fs_pipeline, 'fs_pipeline.joblib')\n",
    "    blob.upload_from_filename('fs_pipeline.joblib')\n",
    "    \n",
    "    Xtrain.to_parquet(Xtrain_fs.path + '.parquet', index=False, compression='gzip')\n",
    "    Xval.to_parquet(Xval_fs.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfecf906-beaa-4195-a33e-984197f1d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    #packages_to_install=requirement_list,\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src@sha256:fdc6b2fed2deac0ea267878cfde028f3b30079b104a6f4047d120c820def1b83\",\n",
    "    output_component_file=\"train_model.yaml\"\n",
    ")\n",
    "\n",
    "def train_model(\n",
    "    Xtrain_fs: Input[Dataset],\n",
    "    Xval_fs: Input[Dataset],\n",
    "    ytrain_: Input[Dataset],\n",
    "    yval_: Input[Dataset],\n",
    "    model_: Output[Model],\n",
    "    metrics_: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import pickle\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.utils as utils\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    from src.guara.modeling.supervised_modelz import SupervisedModelz\n",
    "    \n",
    "    Xtrain = pd.read_parquet(Xtrain_fs.path + \".parquet\")\n",
    "    Xval = pd.read_parquet(Xval_fs.path + \".parquet\")\n",
    "    ytrain = pd.read_parquet(ytrain_.path + \".parquet\")\n",
    "    yval = pd.read_parquet(yval_.path + \".parquet\")\n",
    "    \n",
    "    scale_pos_weight=ytrain.value_counts(normalize=True)[0]/ytrain.value_counts(normalize=True)[1]  \n",
    "    params = {\n",
    "        'random_state':501, \n",
    "        'boosting_type':'gbdt', \n",
    "        'device_type':'cpu',\n",
    "        'scale_pos_weight':scale_pos_weight,\n",
    "        'sub_sample':0.8,\n",
    "        'min_child_samples':24,\n",
    "        'learning_rate':0.38832846505493473,\n",
    "        'colsample_bytree':0.31177546084715557,\n",
    "        'n_estimators':499,\n",
    "        'max_depth':4,\n",
    "        'num_leaves':10\n",
    "    }\n",
    "\n",
    "    md = SupervisedModelz('lgbm', 'binary')\n",
    "    model = md.fit(Xtrain, Xval, ytrain, yval, params)\n",
    "    model_.metadata['framework'] = 'lgbm'\n",
    "    file_name = model_.path + '.pkl'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    # evaluate model performance\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC_AUC']\n",
    "    ytrain_pred = md.predict(Xtrain)\n",
    "    yval_pred = md.predict(Xval)\n",
    "\n",
    "    md.eval_binary(ytrain['target'], ytrain_pred, yval['target'], yval_pred)\n",
    "\n",
    "    if type(ytrain) == pd.Series:\n",
    "        ytrain = ytrain.values\n",
    "    if type(yval) == pd.Series:\n",
    "        yval = yval.values\n",
    "\n",
    "    print('\\n ============== Resumo metricas ============== \\n')\n",
    "    print('TREINO:\\n')\n",
    "    metrics_train = md.metrics_print(\n",
    "        ytrain.values.clip(0, None), \n",
    "        ytrain_pred.clip(0, None), \n",
    "        metrics\n",
    "    )\n",
    "\n",
    "    print('\\nVALIDACAO:\\n')\n",
    "    metrics_val = md.metrics_print(\n",
    "        yval.values.clip(0, None), \n",
    "        yval_pred.clip(0, None), \n",
    "        metrics\n",
    "    )\n",
    "    \n",
    "    # save model artifacts\n",
    "    #-------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name='gpa-churn'\n",
    "    model_file='artifacts/training_pipeline/model/model.joblib'\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(model_file)\n",
    "    joblib.dump(model, 'model.joblib')\n",
    "    blob.upload_from_filename('model.joblib')\n",
    "    \n",
    "    metrics_val.to_parquet(metrics_.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9c9086-b2bd-4382-ad71-aa87407cf263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/components/component_factory.py:260: UserWarning: Could not serialize the default value of the parameter \"f1_threshold\". Failed to serialize the value \"0.6\" of type \"float\" to type \"Integer\". Exception: Value \"0.6\" has type \"<class 'float'>\" instead of int.\n",
      "  ' parameter \"{}\". {}'.format(parameter.name, ex))\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    #packages_to_install=['pandas', 'numpy'],\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src@sha256:fdc6b2fed2deac0ea267878cfde028f3b30079b104a6f4047d120c820def1b83\",\n",
    "    output_component_file=\"evaluate_model.yaml\"\n",
    ")\n",
    "\n",
    "def evaluate_model(\n",
    "    metrics_: Input[Dataset],\n",
    "    f1_threshold:int=0.6\n",
    "    ) -> NamedTuple('output', [('deploy', str)]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    def threshold_check(val1, val2):\n",
    "        cond = \"false\"\n",
    "        if val1 >= val2 :\n",
    "            cond = \"true\"\n",
    "        return cond\n",
    "    \n",
    "    metrics = pd.read_parquet(metrics_.path + '.parquet')\n",
    "    deploy = threshold_check(float(metrics['F1']), float(f1_threshold))\n",
    "    return (deploy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5542cd17-b4c8-4d35-b0c4-ac5471398ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    #packages_to_install=['pandas', 'numpy'],\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src@sha256:fdc6b2fed2deac0ea267878cfde028f3b30079b104a6f4047d120c820def1b83\",\n",
    "    output_component_file=\"sim_deploy.yaml\"\n",
    ")\n",
    "\n",
    "def sim_deploy(model: Input[Model]):\n",
    "    \n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from io import BytesIO\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # reading files\n",
    "    #-------------------------------------------------------\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name='gpa-churn'\n",
    "    artifact_list = ['model', 'fe_pipeline', 'fs_pipeline']\n",
    "    \n",
    "    for artifact_label in artifact_list:\n",
    "        \n",
    "        art_file=f'artifacts/training_pipeline/{artifact_label}/{artifact_label}.joblib'\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(art_file)\n",
    "        art_file = BytesIO()\n",
    "        blob.download_to_file(art_file)\n",
    "        artifact=joblib.load(art_file)\n",
    "        \n",
    "        art_file=f'artifacts/training_pipeline/production/{artifact_label}.joblib'\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(art_file)\n",
    "        joblib.dump(artifact, 'artifact-local.joblib')\n",
    "        blob.upload_from_filename('artifact-local.joblib')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f22fc-90a6-418b-9c24-dfc8e0faff2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f4070a0-e504-4246-8467-b60591c95ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pipeline\n",
    "from datetime import datetime\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-test-{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f6b93de-8664-4a23-a12a-1d25c1b92671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=\"pipeline-train-churn\")\n",
    "\n",
    "def pipeline(\n",
    "    prefix: str = 'gs://gpa-churn/data/processed/input/',\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    #api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    f1_threshold: float = 0.6,\n",
    "    serving_container_image_uri: str = \"gcr.io/gpa-poc-001/churn-base-image@sha256:c18908472b7cc7663502e660943ea6c2e5c0a14054cdb20f762f37b7b5df55d7:latest\"\n",
    "    ):\n",
    "    \n",
    "    data_op = get_preprocessed_data(\n",
    "        prefix='gs://gpa-churn/data/processed/input/'\n",
    "        )\n",
    "    \n",
    "    fe_pipe_op = feature_engineering_sequence(\n",
    "        data_op.outputs['Xtrain_'],\n",
    "        data_op.outputs['Xval_']\n",
    "        )\n",
    "    \n",
    "    fs_pipe_op = feature_selection_sequence(\n",
    "        fe_pipe_op.outputs['Xtrain_fe'],\n",
    "        fe_pipe_op.outputs['Xval_fe'],\n",
    "        data_op.outputs['ytrain_'],\n",
    "        data_op.outputs['yval_']\n",
    "        )\n",
    "    \n",
    "    train_model_op = train_model(\n",
    "        fs_pipe_op.outputs['Xtrain_fs'],\n",
    "        fs_pipe_op.outputs['Xval_fs'],\n",
    "        data_op.outputs['ytrain_'],\n",
    "        data_op.outputs['yval_']\n",
    "        )\n",
    "    \n",
    "    model_evaluation_op = evaluate_model(\n",
    "        train_model_op.outputs['metrics_'],\n",
    "        0.6\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        model_evaluation_op.outputs['deploy']=='true',\n",
    "        name=\"deploy-condition\",\n",
    "    ):\n",
    "           \n",
    "        deploy_model_op = sim_deploy(train_model_op.outputs['model_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91545e9d-ca9a-4fe1-be79-f7f717dff4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_pipeline_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a47ca6-608a-488e-80a3-30a94dba0b41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fab0af9f-45d8-4376-9ac0-a3dbbee75940",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"churn-test-pipeline\",\n",
    "    template_path=\"ml_pipeline_test.json\",\n",
    "    enable_caching=True,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150ab38b-1ea0-4c91-bb5a-e75d314053c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/southamerica-east1/pipelines/runs/pipeline-train-churn-20220602125836?project=437364709834\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-train-churn-20220602125836 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [evaluate-model].; Job (project_id = gpa-poc-001, job_id = 2714333569165033472) is failed due to the above error.; Failed to handle the job: {project_number = 437364709834, job_id = 2714333569165033472}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3821/1043187068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    265\u001b[0m         )\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     def submit(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [evaluate-model].; Job (project_id = gpa-poc-001, job_id = 2714333569165033472) is failed due to the above error.; Failed to handle the job: {project_number = 437364709834, job_id = 2714333569165033472}\"\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
