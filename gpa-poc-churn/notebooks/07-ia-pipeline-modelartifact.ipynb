{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005c50df-c4c1-405e-93b5-10175356de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter_contrib_nbextensions\n",
    "# !jupyter contrib nbextension install - user\n",
    "# from jedi import settings\n",
    "# settings.case_insensitive_completion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67443260-a0bc-4605-b53c-be64ae15d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install ai platform and kfp\n",
    "# USER_FLAG = \"--user\"\n",
    "# !pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
    "# !pip3 install {USER_FLAG} kfp --upgrade\n",
    "# !pip install google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88044735-902b-41ef-8f4c-2e01821eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d591c5b2-2d78-4d10-9363-b4a3aff21951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud services enable compute.googleapis.com         \\\n",
    "#                        containerregistry.googleapis.com  \\\n",
    "#                        aiplatform.googleapis.com  \\\n",
    "#                        cloudbuild.googleapis.com \\\n",
    "#                        cloudfunctions.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19aaf51-d63d-4771-92f0-45c356bc586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ab6367-523a-4b42-a5da-123405dc2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\"\n",
    "#!gcloud auth login if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af458713-51b7-465f-be5d-e76d5c2d1de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpa-poc-001'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412eb35a-3148-4af2-97e1-a31e1b7064b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://gpa-churn/artifacts/pipeline-vertexai/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://gpa-churn/artifacts\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline-vertexai/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fe1b1b-fc92-442a-81cb-797b0c3adab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'southamerica-east1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGION=\"southamerica-east1\"\n",
    "REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20534a88-4c71-4959-a3d3-ebd434803ebd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ff9151-db05-4b51-a99a-c05a0f6fb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"get_preprocessed_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_preprocessed_data(\n",
    "    Xtrain_: Output[Dataset],\n",
    "    Xval_: Output[Dataset],\n",
    "    ytrain_: Output[Dataset],\n",
    "    yval_: Output[Dataset],\n",
    "    prefix:str='gs://gpa-churn/data/processed/input/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import gc\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    bucket = prefix.split('/')[2]\n",
    "    storage_client = storage.Client()\n",
    "    obj_list = storage_client.list_blobs(bucket)\n",
    "    obj_list = [i.name for i in obj_list if 'data/processed/input/' in i.name]\n",
    "    obj_list = obj_list[1:]\n",
    "    df_list = []\n",
    "    for obj in obj_list:\n",
    "        local_df = pd.read_parquet('gs://gpa-churn/'+obj)\n",
    "        df_list.append(local_df)\n",
    "        print(f'added {prefix}{obj}')\n",
    "\n",
    "    df = pd.concat(df_list, axis=0)\n",
    "    df.drop(columns=['cod_cliente'], inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    target = 'target'\n",
    "    features = list(df.columns)\n",
    "    features = [i for i in features if i != target]\n",
    "\n",
    "    Xtrain, Xval, ytrain, yval = train_test_split(\n",
    "        df[features], \n",
    "        df[[target]],\n",
    "        test_size=0.15, \n",
    "        random_state=501\n",
    "        )\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    Xtrain.reset_index(drop=True, inplace=True)\n",
    "    Xval.reset_index(drop=True, inplace=True)\n",
    "    ytrain.reset_index(drop=True, inplace=True)\n",
    "    yval.reset_index(drop=True, inplace=True)\n",
    "    print('Successfully read training data')\n",
    "    print('shapes:')\n",
    "    print(f'xtrain:{Xtrain.shape}, ytrain:{ytrain.shape}')\n",
    "    print(f'xval:{Xval.shape}, yval:{yval.shape}')\n",
    "    \n",
    "    Xtrain.to_parquet(Xtrain_.path + '.parquet', index=False, compression='gzip')\n",
    "    Xval.to_parquet(Xval_.path + '.parquet', index=False, compression='gzip')\n",
    "    ytrain.to_parquet(ytrain_.path + '.parquet', index=False, compression='gzip')\n",
    "    yval.to_parquet(yval_.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3260a28e-d24f-44c0-a77d-68f3cab71b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"feature_engineering_sequence.yaml\"\n",
    ")\n",
    "\n",
    "def feature_engineering_sequence(\n",
    "    Xtrain_: Input[Dataset],\n",
    "    Xval_: Input[Dataset],\n",
    "    Xtrain_fe: Output[Dataset],\n",
    "    Xval_fe: Output[Dataset],\n",
    "    fe_pipeline_: Output[Model]\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    Xtrain = pd.read_parquet(Xtrain_.path + \".parquet\")\n",
    "    Xval = pd.read_parquet(Xval_.path + \".parquet\")\n",
    "    \n",
    "    numerical_columns = [\n",
    "        'val_venda_bruta_cupom',\n",
    "        'qtd_item_venda',\n",
    "        'flg_vend_meu_desct',\n",
    "        'valor_desconto',\n",
    "        'flag_dev',\n",
    "        'tipo_promo_0',\n",
    "        'tipo_promo_1',\n",
    "        'tipo_promo_2',\n",
    "        'tipo_promo_3',\n",
    "        'tipo_promo_4',\n",
    "        'tipo_promo_5',\n",
    "        'categoria_0',\n",
    "        'categoria_1',\n",
    "        'categoria_2',\n",
    "        'categoria_3',\n",
    "        'categoria_4',\n",
    "        'categoria_5',\n",
    "        'categoria_6',\n",
    "        'categoria_7',\n",
    "        'departamento_0',\n",
    "        'compras_mes',\n",
    "        'agg_l3m_val_venda_bruta_cupom',\n",
    "        'agg_l3m_qtd_item_venda',\n",
    "        'agg_l3m_flg_vend_meu_desct',\n",
    "        'agg_l3m_valor_desconto',\n",
    "        'agg_l3m_flag_dev',\n",
    "        'agg_l3m_tipo_promo_0',\n",
    "        'agg_l3m_tipo_promo_1',\n",
    "        'agg_l3m_tipo_promo_2',\n",
    "        'agg_l3m_tipo_promo_3',\n",
    "        'agg_l3m_tipo_promo_4',\n",
    "        'agg_l3m_tipo_promo_5',\n",
    "        'agg_l3m_categoria_0',\n",
    "        'agg_l3m_categoria_1',\n",
    "        'agg_l3m_categoria_2',\n",
    "        'agg_l3m_categoria_3',\n",
    "        'agg_l3m_categoria_4',\n",
    "        'agg_l3m_categoria_5',\n",
    "        'agg_l3m_categoria_6',\n",
    "        'agg_l3m_categoria_7',\n",
    "        'agg_l3m_departamento_0',\n",
    "        'agg_l3m_compras_mes',\n",
    "    ]\n",
    "\n",
    "    outlier_columns_mean = [\n",
    "        'pib_percapita',\n",
    "        'idade',\n",
    "        'delta_de_cadastro',\n",
    "        'delta_de_stix'\n",
    "    ]\n",
    "\n",
    "    yeojohnson_columns = [\n",
    "        'val_venda_bruta_cupom',\n",
    "        'qtd_item_venda',\n",
    "        'flg_vend_meu_desct',\n",
    "        'valor_desconto',\n",
    "        'compras_mes',\n",
    "        'agg_l3m_val_venda_bruta_cupom',\n",
    "        'agg_l3m_qtd_item_venda',\n",
    "        'agg_l3m_flg_vend_meu_desct',\n",
    "        'agg_l3m_valor_desconto',\n",
    "        'agg_l3m_compras_mes',\n",
    "        'pib_percapita',\n",
    "        'idade',\n",
    "        'delta_de_cadastro'\n",
    "    ]\n",
    "    \n",
    "    # training set\n",
    "    #-------------------------------------------------------\n",
    "    fe_pipeline = Pipeline([\n",
    "        ('drop_temporary_columns', pipeline_modules.drop_temporary_columns()),\n",
    "        ('drop_with_low_variance', pipeline_modules.drop_numerical_with_variance(columns=numerical_columns)),\n",
    "        ('encode_sex_column', pipeline_modules.encode_sex_column()),\n",
    "        ('group_rare_regions', pipeline_modules.group_rare_categorical(columns=['region'], threshold=0.002)),\n",
    "        ('encode_regions', pipeline_modules.encode_categorical(columns=['region'])),\n",
    "        ('handle_outliers_max', pipeline_modules.outlier_handling(\n",
    "            columns=numerical_columns, \n",
    "            method='gauss', \n",
    "            band=2.8, \n",
    "            action='max')),\n",
    "        ('handle_outliers_mean', pipeline_modules.outlier_handling(\n",
    "            columns=outlier_columns_mean, \n",
    "            method='gauss', \n",
    "            band=2.5, \n",
    "            action='mean')),\n",
    "        ('handle_negative_values', pipeline_modules.handle_negative_values(columns=numerical_columns)),\n",
    "        ('fill_missing_numerical_zero', pipeline_modules.fill_na_values_with_zero(\n",
    "            columns=['ind_email','cadastro_stix','delta_de_cadastro','delta_de_stix'])),\n",
    "        ('fill_missing_numerical_mean', pipeline_modules.fill_na_values_with_zero(\n",
    "            columns=['pib_percapita','idade'])),\n",
    "        ('transform_yeojohnson', pipeline_modules.data_transformation(\n",
    "            columns=yeojohnson_columns, \n",
    "            method='yeojohnson'))\n",
    "    ])\n",
    "\n",
    "    Xtrain = fe_pipeline.fit_transform(Xtrain)\n",
    "\n",
    "    # validation set\n",
    "    #-------------------------------------------------------\n",
    "    Xval = fe_pipeline.transform(Xval)\n",
    "    \n",
    "    # save feature engineering artifacts\n",
    "    #-------------------------------------------------------\n",
    "    file_name = fe_pipeline_.path + '.joblib'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        joblib.dump(fe_pipeline, file)\n",
    "    \n",
    "    Xtrain.to_parquet(Xtrain_fe.path + '.parquet', index=False, compression='gzip')\n",
    "    Xval.to_parquet(Xval_fe.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e41c6a4-0b4b-43da-9b8a-5be87b0bb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"feature_selection_sequence.yaml\"\n",
    ")\n",
    "\n",
    "def feature_selection_sequence(\n",
    "    Xtrain_fe: Input[Dataset],\n",
    "    Xval_fe: Input[Dataset],\n",
    "    ytrain_: Input[Dataset],\n",
    "    yval_: Input[Dataset],\n",
    "    Xtrain_fs: Output[Dataset],\n",
    "    Xval_fs: Output[Dataset],\n",
    "    fs_pipeline_: Output[Model],\n",
    "    baseline_df_: Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.utils as utils\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    Xtrain = pd.read_parquet(Xtrain_fe.path + \".parquet\")\n",
    "    Xval = pd.read_parquet(Xval_fe.path + \".parquet\")\n",
    "    ytrain = pd.read_parquet(ytrain_.path + \".parquet\")\n",
    "    yval = pd.read_parquet(yval_.path + \".parquet\")\n",
    "    \n",
    "    # training set\n",
    "    #-------------------------------------------------------\n",
    "    fs_pipeline = Pipeline([\n",
    "            ('select_with_correlation', pipeline_modules.select_with_correlation(\n",
    "                threshold=0.82, \n",
    "                method='recursive',\n",
    "                objective='classification'))\n",
    "        ])\n",
    "        \n",
    "    Xtrain = fs_pipeline.fit_transform(Xtrain, ytrain)\n",
    "    \n",
    "    # validation set\n",
    "    #-------------------------------------------------------\n",
    "    Xval = fs_pipeline.transform(Xval)\n",
    "    \n",
    "    # create baseline for model monitoring\n",
    "    #-------------------------------------------------------\n",
    "    baseline_df = pd.concat([Xtrain, ytrain], axis=1)\n",
    "    \n",
    "    # save feature selection artifacts\n",
    "    #-------------------------------------------------------\n",
    "    file_name = fs_pipeline_.path + '.joblib'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        joblib.dump(fs_pipeline, file)\n",
    "    \n",
    "    # save pipeline datasets\n",
    "    #-------------------------------------------------------\n",
    "    Xtrain.to_parquet(Xtrain_fs.path + '.parquet', index=False, compression='gzip')\n",
    "    Xval.to_parquet(Xval_fs.path + '.parquet', index=False, compression='gzip')\n",
    "    baseline_df.to_parquet(baseline_df_.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfecf906-beaa-4195-a33e-984197f1d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"train_model.yaml\"\n",
    ")\n",
    "\n",
    "def train_model(\n",
    "    Xtrain_fs: Input[Dataset],\n",
    "    Xval_fs: Input[Dataset],\n",
    "    ytrain_: Input[Dataset],\n",
    "    yval_: Input[Dataset],\n",
    "    model_: Output[Model],\n",
    "    metrics_: Output[Dataset],\n",
    "    bucket:str='gpa-churn',\n",
    "    artifact_path:str='artifacts/training_pipeline/xgb/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.utils as utils\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    from src.guara.modeling.supervised_modelz import SupervisedModelz\n",
    "    \n",
    "    Xtrain = pd.read_parquet(Xtrain_fs.path + \".parquet\")\n",
    "    Xval = pd.read_parquet(Xval_fs.path + \".parquet\")\n",
    "    ytrain = pd.read_parquet(ytrain_.path + \".parquet\")\n",
    "    yval = pd.read_parquet(yval_.path + \".parquet\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(Xtrain, ytrain)\n",
    "    dval = xgb.DMatrix(Xval, yval)\n",
    "    \n",
    "    scale_pos_weight=ytrain.value_counts(normalize=True)[0]/ytrain.value_counts(normalize=True)[1]     \n",
    "    params = {\n",
    "        'objective':'binary:logistic',\n",
    "        'gamma': 1, \n",
    "        'verbosity': 0, \n",
    "        'scale_pos_weight': 1.0, \n",
    "        'eta': 0.32924394564404313, \n",
    "        'colsample_bytree': 0.6997715470767337, \n",
    "        'num_iterations': 259.98061008076706, \n",
    "        'lambda': 9.840799645070883, \n",
    "        'n_estimators': 372, \n",
    "        'max_depth': 5, \n",
    "        'feature_fraction': 0,\n",
    "        'eval_metric':'auc',\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    }\n",
    "    \n",
    "    bst = xgb.train(params, dtrain, 20)\n",
    "    \n",
    "    # evaluate model performance\n",
    "    #-------------------------------------------------------\n",
    "    predicted_yval = bst.predict(dval)\n",
    "    predicted_yval = [1 if i>0.54 else 0 for i in predicted_yval]\n",
    "    metrics_dict = {\n",
    "        'roc':float(bst.eval_set([(dval, '0')]).split(':')[1]),\n",
    "        'precision':precision_score(yval, predicted_yval),\n",
    "        'recall':recall_score(yval, predicted_yval),\n",
    "        'f1':f1_score(yval, predicted_yval)\n",
    "    }\n",
    "    metrics = pd.DataFrame(metrics_dict, index=[0])\n",
    "    \n",
    "    # save model performance metrics\n",
    "    #------------------------------------------------------- \n",
    "    metrics.to_parquet(metrics_.path + '.parquet', index=False, compression='gzip')\n",
    "    \n",
    "    # save model artifacts locally\n",
    "    #-------------------------------------------------------\n",
    "    bst.save_model('model.bst')\n",
    "    \n",
    "    # upload local model to cloud storage\n",
    "    #-------------------------------------------------------\n",
    "    bucket_name=bucket\n",
    "    model_file=artifact_path + 'model.bst'\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(model_file)\n",
    "    blob.upload_from_filename('model.bst')\n",
    "    \n",
    "    # save model as pipeline artifact\n",
    "    #-------------------------------------------------------\n",
    "    model_.metadata['framework'] = 'xgb'\n",
    "    bst.save_model(model_.path + '.bst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9c9086-b2bd-4382-ad71-aa87407cf263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"evaluate_model.yaml\"\n",
    ")\n",
    "\n",
    "def evaluate_model(\n",
    "    metrics_: Input[Dataset],\n",
    "    roc_threshold:float=0.80\n",
    "    ) -> NamedTuple('output', [('deploy', str)]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    metrics = pd.read_parquet(metrics_.path + '.parquet')\n",
    "    cond = \"false\"\n",
    "    if float(metrics['roc'].iloc[0]) >= float(roc_threshold):\n",
    "        cond=\"true\"\n",
    "        \n",
    "    return (cond,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5542cd17-b4c8-4d35-b0c4-ac5471398ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"deploy_endpoint.yaml\"\n",
    ")\n",
    "\n",
    "def deploy_endpoint(\n",
    "    model_: Input[Model],\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "    endpoint_information_: Output[Model],\n",
    "    project_id:str='gpa-poc-001',\n",
    "    model_label:str='churn',\n",
    "    region:str=\"us-central1\",\n",
    "    container_uri:str=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest\",\n",
    "    artifact_uri:str='gs://gpa-churn/artifacts/training_pipeline/xgb/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import gc\n",
    "    import sys\n",
    "    import json\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    endpoint_name = f'{model_label}-endpoint'\n",
    "    display_name = f'{model_label}-xgb'\n",
    "    model_name = f'{model_label}-xgb'\n",
    "    \n",
    "    # Create endpoint\n",
    "    #-------------------------------------------------------\n",
    "    endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(endpoint_name),\n",
    "        order_by='create_time desc',\n",
    "        project=project_id, \n",
    "        location=region,\n",
    "        )\n",
    "\n",
    "    if len(endpoints) > 0:\n",
    "        endpoint = endpoints[0]  # most recently created\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=endpoint_name, project=project_id, location=region)\n",
    "    \n",
    "    # Import a model programmatically\n",
    "    #-------------------------------------------------------\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name = display_name, \n",
    "        artifact_uri = artifact_uri,\n",
    "        serving_container_image_uri =  container_uri,\n",
    "        serving_container_health_route=f\"/v1/models/{model_name}\",\n",
    "        serving_container_predict_route=f\"/v1/models/{model_name}:predict\",\n",
    "        serving_container_environment_variables={\n",
    "        \"model_name\": model_name,\n",
    "        },       \n",
    "        )\n",
    "    \n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=\"n1-standard-4\", \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=display_name,\n",
    "        )\n",
    "\n",
    "    # Save data to the output params\n",
    "    #-------------------------------------------------------\n",
    "    vertex_model.uri = model_deploy.resource_name\n",
    "    \n",
    "    # Save endpoint.resource_name for prediction reference\n",
    "    #-------------------------------------------------------\n",
    "    endpoint_information_dict = {\n",
    "        'project_number':str(model_deploy.resource_name.split('/')[1]),\n",
    "        'endpoint':str(model_deploy.resource_name.split('/')[-1])\n",
    "    }\n",
    "    with open(endpoint_information_.path+'.json', 'w') as file:\n",
    "        json.dump(endpoint_information_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9f69663-e93a-456c-ad15-84b0b22c8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb-gcloud@sha256:0985f7e13d3d1f234462e7e0b32f8e2563c0ca312006a4361e593e27a43bce5c\",\n",
    "    output_component_file=\"deploy_model_monitoring_job.yaml\"\n",
    ")\n",
    "\n",
    "def deploy_model_monitoring_job(\n",
    "    baseline_df_: Input[Dataset],\n",
    "    endpoint_information_: Input[Model],\n",
    "    model_monitor_information_: Output[Model],\n",
    "    region:str='us-central1',\n",
    "    baseline_path:str='gs://gpa-churn/artifacts/training_pipeline/baseline/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    # reading inputs\n",
    "    #-------------------------------------------------------\n",
    "    baseline_df = pd.read_parquet(baseline_df_.path+'.parquet')\n",
    "    with open(endpoint_information_.path+'.json', 'r') as file:\n",
    "        endpoint_information = json.load(file)\n",
    "    endpoint = endpoint_information['endpoint']\n",
    "    \n",
    "    # saving baseline for model monitor in cloud storage\n",
    "    #-------------------------------------------------------\n",
    "    baseline_df.to_csv(baseline_path+'data.csv', index=False)\n",
    "    \n",
    "    # creating feature_thresh_description from feature_list\n",
    "    #-------------------------------------------------------\n",
    "    feature_list = list(baseline_df.columns) # ['feature1', 'feature2', ..., 'target']\n",
    "    feature_list = [i for i in feature_list if i != 'target']\n",
    "    feature_thresh_description = ''.join(i+'=0.3,' for i in feature_list)[0:-1]\n",
    "    \n",
    "    # deploy or update model monitoring job\n",
    "    #-------------------------------------------------------\n",
    "    sdk_command = f'gcloud beta ai model-monitoring-jobs list --region={region} --project=gpa-poc-001'\n",
    "    os_string_output = os.popen(sdk_command).read()\n",
    "    if os_string_output != '':\n",
    "        mmjob_name = os_string_output.split('job-')[1].split('/')[0].split('\\n')[0]\n",
    "        update_mmjob_command = f'gcloud beta ai model-monitoring-jobs update ({mmjob_name} --region={region} -- project=gpa-poc-001) \\\n",
    "        --emails=italo.avila@tenbu.com.br \\\n",
    "        --endpoint={endpoint} \\\n",
    "        --prediction-sampling-rate=0.5 \\\n",
    "        --monitoring-frequency=1 \\\n",
    "        --target-field=target \\\n",
    "        --training-sampling-rate=1.0 \\\n",
    "        --data-format=csv \\\n",
    "        --gcs-uris=gs://gpa-churn/artifacts/training_pipeline/baseline/data.csv \\\n",
    "        --feature-thresholds={feature_thresh_description}'\n",
    "        os.system(update_mmjob_command)\n",
    "    else:\n",
    "        print(f'No model monitoring jobs deployed in region={region}')\n",
    "        deploy_mmjob_command = f'gcloud beta ai model-monitoring-jobs create --display-name=churn-model-monitor \\\n",
    "        --project=gpa-poc-001 \\\n",
    "        --emails=italo.avila@tenbu.com.br \\\n",
    "        --endpoint={endpoint} \\\n",
    "        --prediction-sampling-rate=0.5 \\\n",
    "        --monitoring-frequency=1 \\\n",
    "        --region={region} \\\n",
    "        --target-field=target \\\n",
    "        --training-sampling-rate=1.0 \\\n",
    "        --data-format=csv \\\n",
    "        --gcs-uris=gs://gpa-churn/artifacts/training_pipeline/baseline/data.csv \\\n",
    "        --feature-thresholds={feature_thresh_description}'\n",
    "        os.system(deploy_mmjob_command)\n",
    "    \n",
    "    # save model monitor information\n",
    "    #-------------------------------------------------------\n",
    "    os_string_output = os.popen(sdk_command).read()\n",
    "    mmjob_name = os_string_output.split('job-')[1].split('/')[0].split('\\n')[0]\n",
    "    model_monitor_information = {\n",
    "        'project':'gpa-poc-001',\n",
    "        'region':region,\n",
    "        'model_monitor_id':mmjob_name\n",
    "    }\n",
    "    with open(model_monitor_information_.path+'.json', 'w') as file:\n",
    "        json.dump(model_monitor_information, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b1abe5-c117-45fc-8e8b-eb6ea686090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"save_consolidated_artifacts.yaml\"\n",
    ")\n",
    "\n",
    "def save_consolidated_artifacts(\n",
    "    model_: Input[Model],\n",
    "    fe_pipeline_: Input[Model],\n",
    "    fs_pipeline_: Input[Model],\n",
    "    endpoint_information_: Input[Model],\n",
    "    model_monitor_information_: Input[Model],\n",
    "    metrics_: Input[Dataset],\n",
    "    bucket:str='gpa-churn',\n",
    "    consolidated_artifacts_path:str='artifacts/training_pipeline/production/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import gc\n",
    "    import sys\n",
    "    import json\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.utils as utils\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    # loading artifacts\n",
    "    #-------------------------------------------------------\n",
    "    bst = xgb.Booster()\n",
    "    bst.load_model(model_.path+'.bst')\n",
    "    fe_pipeline = joblib.load(fe_pipeline_.path+'.joblib')\n",
    "    fs_pipeline = joblib.load(fs_pipeline_.path+'.joblib')\n",
    "    with open(endpoint_information_.path+'.json', 'r') as file:\n",
    "        endpoint_information = json.load(file)\n",
    "    with open(model_monitor_information_.path+'.json', 'r') as file:\n",
    "        model_monitor_information = json.load(file)\n",
    "    metrics = pd.read_parquet(metrics_.path+'.parquet')\n",
    "    \n",
    "    # getting bucket\n",
    "    #-------------------------------------------------------\n",
    "    storage_client = storage.Client()\n",
    "    bucket_ = storage_client.get_bucket(bucket)\n",
    "    \n",
    "    # saving pipelines in artifact path\n",
    "    #-------------------------------------------------------\n",
    "    pipe_list = [fe_pipeline, fs_pipeline]\n",
    "    pipe_label_list = ['fe_pipeline.joblib', 'fs_pipeline.joblib']\n",
    "    for i in range(len(pipe_list)):\n",
    "        art_file=f'{consolidated_artifacts_path}{pipe_label_list[i]}'\n",
    "        blob = bucket_.blob(art_file)\n",
    "        joblib.dump(pipe_list[i], pipe_label_list[i])\n",
    "        blob.upload_from_filename(pipe_label_list[i])\n",
    "    \n",
    "    # saving endpoint_information in artifact path\n",
    "    #-------------------------------------------------------\n",
    "    file_name = 'endpoint_information.json'\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(endpoint_information, file)\n",
    "    art_file=f'{consolidated_artifacts_path}{file_name}'\n",
    "    blob = bucket_.blob(art_file)\n",
    "    blob.upload_from_filename(file_name)\n",
    "    \n",
    "    # saving model_monitoring_information in artifact path\n",
    "    #-------------------------------------------------------\n",
    "    file_name = 'model_monitor_information.json'\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(model_monitor_information, file)\n",
    "    art_file=f'{consolidated_artifacts_path}{file_name}'\n",
    "    blob = bucket_.blob(art_file)\n",
    "    blob.upload_from_filename(file_name)\n",
    "    \n",
    "    # upload local model to cloud storage\n",
    "    #-------------------------------------------------------\n",
    "    file_name = 'model.bst'\n",
    "    bst.save_model(file_name)\n",
    "    art_file=f'{consolidated_artifacts_path}{file_name}'\n",
    "    blob = bucket_.blob(art_file)\n",
    "    blob.upload_from_filename(file_name)\n",
    "    \n",
    "    # saving metrics\n",
    "    #-------------------------------------------------------\n",
    "    file_name = 'metrics.parquet'\n",
    "    metrics_path = f'gs://{bucket}/{consolidated_artifacts_path}{file_name}'\n",
    "    metrics.to_parquet(metrics_path, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f22fc-90a6-418b-9c24-dfc8e0faff2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f4070a0-e504-4246-8467-b60591c95ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pipeline\n",
    "from datetime import datetime\n",
    "timestamp=datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "pipeline_label = f'pipeline-churn-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f6b93de-8664-4a23-a12a-1d25c1b92671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=pipeline_label)\n",
    "\n",
    "def pipeline(\n",
    "    bucket:str='gpa-churn',\n",
    "    artifact_path:str='artifacts/training_pipeline/xgb/',\n",
    "    project_id:str='gpa-poc-001',\n",
    "    region:str=\"us-central1\", \n",
    "    model_label:str='churn',\n",
    "    roc_threshold: float=0.8,\n",
    "    model_serving_image:str=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest\",\n",
    "    consolidated_artifacts_path:str='artifacts/training_pipeline/production/'\n",
    "    ):\n",
    "    \n",
    "    data_op = get_preprocessed_data(\n",
    "        prefix=f'gs://{bucket}/data/processed/input/'\n",
    "        )\n",
    "    \n",
    "    fe_pipe_op = feature_engineering_sequence(\n",
    "        data_op.outputs['Xtrain_'],\n",
    "        data_op.outputs['Xval_']\n",
    "        )\n",
    "    \n",
    "    fs_pipe_op = feature_selection_sequence(\n",
    "        fe_pipe_op.outputs['Xtrain_fe'],\n",
    "        fe_pipe_op.outputs['Xval_fe'],\n",
    "        data_op.outputs['ytrain_'],\n",
    "        data_op.outputs['yval_'],\n",
    "        )\n",
    "    \n",
    "    train_model_op = train_model(\n",
    "        fs_pipe_op.outputs['Xtrain_fs'],\n",
    "        fs_pipe_op.outputs['Xval_fs'],\n",
    "        data_op.outputs['ytrain_'],\n",
    "        data_op.outputs['yval_'],\n",
    "        bucket=bucket,\n",
    "        artifact_path=artifact_path\n",
    "        )\n",
    "    \n",
    "    model_evaluation_op = evaluate_model(\n",
    "        train_model_op.outputs['metrics_'],\n",
    "        roc_threshold=roc_threshold\n",
    "        )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        model_evaluation_op.outputs['deploy']=='true',\n",
    "        name=\"deploy-endpoint\",\n",
    "        ):\n",
    "        \n",
    "        deploy_model_op = deploy_endpoint(\n",
    "            train_model_op.outputs['model_'],\n",
    "            project_id=project_id,\n",
    "            model_label=model_label,\n",
    "            region=region,\n",
    "            container_uri=model_serving_image,\n",
    "            artifact_uri=f'gs://{bucket}/{artifact_path}'\n",
    "            )\n",
    "        \n",
    "        deploy_monitor_op = deploy_model_monitoring_job(\n",
    "            fs_pipe_op.outputs['baseline_df_'],\n",
    "            deploy_model_op.outputs['endpoint_information_'],\n",
    "            region=region,\n",
    "            baseline_path=f'gs://{bucket}/artifacts/training_pipeline/baseline/'\n",
    "            )\n",
    "        \n",
    "        save_consolidated_artifacts(\n",
    "            train_model_op.outputs['model_'],\n",
    "            fe_pipe_op.outputs['fe_pipeline_'],\n",
    "            fs_pipe_op.outputs['fs_pipeline_'],\n",
    "            deploy_model_op.outputs['endpoint_information_'],\n",
    "            deploy_monitor_op.outputs['model_monitor_information_'],\n",
    "            train_model_op.outputs['metrics_'],\n",
    "            bucket=bucket,\n",
    "            consolidated_artifacts_path=consolidated_artifacts_path\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91545e9d-ca9a-4fe1-be79-f7f717dff4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a47ca6-608a-488e-80a3-30a94dba0b41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab0af9f-45d8-4376-9ac0-a3dbbee75940",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=pipeline_label,\n",
    "    template_path=\"ml_pipeline.json\",\n",
    "    enable_caching=True,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "150ab38b-1ea0-4c91-bb5a-e75d314053c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/southamerica-east1/pipelines/runs/pipeline-churn-20220609142225?project=437364709834\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-20220609142225\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
