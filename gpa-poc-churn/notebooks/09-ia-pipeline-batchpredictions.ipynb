{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005c50df-c4c1-405e-93b5-10175356de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter_contrib_nbextensions\n",
    "# !jupyter contrib nbextension install - user\n",
    "# from jedi import settings\n",
    "# settings.case_insensitive_completion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67443260-a0bc-4605-b53c-be64ae15d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install ai platform and kfp\n",
    "# USER_FLAG = \"--user\"\n",
    "# !pip3 install {USER_FLAG} google-cloud-aiplatform==1.3.0 --upgrade\n",
    "# !pip3 install {USER_FLAG} kfp --upgrade\n",
    "# !pip install google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88044735-902b-41ef-8f4c-2e01821eb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d591c5b2-2d78-4d10-9363-b4a3aff21951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud services enable compute.googleapis.com         \\\n",
    "#                        containerregistry.googleapis.com  \\\n",
    "#                        aiplatform.googleapis.com  \\\n",
    "#                        cloudbuild.googleapis.com \\\n",
    "#                        cloudfunctions.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19aaf51-d63d-4771-92f0-45c356bc586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ab6367-523a-4b42-a5da-123405dc2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\"\n",
    "#!gcloud auth login if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af458713-51b7-465f-be5d-e76d5c2d1de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpa-poc-001'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412eb35a-3148-4af2-97e1-a31e1b7064b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://gpa-churn/artifacts/pipeline-vertexai/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://gpa-churn/artifacts\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline-vertexai/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fe1b1b-fc92-442a-81cb-797b0c3adab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'southamerica-east1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGION=\"southamerica-east1\"\n",
    "REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20534a88-4c71-4959-a3d3-ebd434803ebd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ff9151-db05-4b51-a99a-c05a0f6fb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"get_prediction_data.yaml\"\n",
    ")\n",
    "\n",
    "def get_prediction_data(\n",
    "    Xpred_: Output[Dataset],\n",
    "    cod_cliente_: Output[Dataset],\n",
    "    df_date_: Output[Dataset],\n",
    "    path:str='gs://gpa-churn/data/processed/input/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import gc\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # extracting bucket and path info from path\n",
    "    #-------------------------------------------------------\n",
    "    bucket = path.split('/')[2]\n",
    "    path_ref = '/'.join(i for i in path.split('/')[3:-1])\n",
    "    \n",
    "    # reading dataframes in path folder\n",
    "    #-------------------------------------------------------\n",
    "    storage_client = storage.Client()\n",
    "    obj_list = storage_client.list_blobs(bucket)\n",
    "    obj_list = [i.name for i in obj_list if path_ref in i.name]\n",
    "    obj_list = obj_list[1:]\n",
    "    df_list = []\n",
    "    for obj in obj_list:\n",
    "        local_df = pd.read_parquet(f'gs://{bucket}/{obj}')\n",
    "        df_list.append(local_df)\n",
    "        print(f'added {path}{obj}')\n",
    "        \n",
    "    # concatenating df_list and saving cod_client column in an independent df\n",
    "    #-------------------------------------------------------\n",
    "    df = pd.concat(df_list, axis=0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    cod_cliente = df[['cod_cliente']].copy()\n",
    "    df_date = df[['date']].copy()\n",
    "    \n",
    "    # selecting valid columns\n",
    "    #-------------------------------------------------------\n",
    "    df.drop(columns=['cod_cliente'], inplace=True)\n",
    "    target = 'target'\n",
    "    features = list(df.columns)\n",
    "    features = [i for i in features if i != target]\n",
    "    Xpred = df[features]\n",
    "    print('Successfully read prediction data.')\n",
    "    print('shapes:')\n",
    "    print(f'xtrain:{Xpred.shape}')\n",
    "    \n",
    "    # saving output datasets in pipeline\n",
    "    #-------------------------------------------------------\n",
    "    Xpred.to_parquet(Xpred_.path + '.parquet', index=False, compression='gzip')\n",
    "    cod_cliente.to_parquet(cod_cliente_.path + '.parquet', index=False, compression='gzip')\n",
    "    df_date.to_parquet(df_date_.path + '.parquet', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3260a28e-d24f-44c0-a77d-68f3cab71b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"load_artifacts.yaml\"\n",
    ")\n",
    "\n",
    "def load_artifacts(\n",
    "    fe_pipeline_: Output[Model],\n",
    "    fs_pipeline_: Output[Model],\n",
    "    model_: Output[Model],\n",
    "    path:str='gs://gpa-churn/artifacts/training_pipeline/production/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import pytz\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from io import BytesIO\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    # extracting bucket and path info from prefix\n",
    "    #-------------------------------------------------------\n",
    "    bucket_name = path.split('/')[2]\n",
    "    path_ref = '/'.join(i for i in path.split('/')[3:-1])\n",
    "    \n",
    "    # creating storage access point\n",
    "    #-------------------------------------------------------\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \n",
    "    # reading pipeline artifacts\n",
    "    #-------------------------------------------------------\n",
    "    pipe_dict = {\n",
    "        'fe_pipeline':None,\n",
    "        'fs_pipeline':None\n",
    "    }\n",
    "\n",
    "    for pipe in pipe_dict.keys():\n",
    "        art_file=f'{path_ref}/{pipe}.joblib'\n",
    "        blob = bucket.blob(art_file)\n",
    "        art_obj = BytesIO()\n",
    "        blob.download_to_file(art_obj)\n",
    "        pipe_dict[pipe]=joblib.load(art_obj)\n",
    "    \n",
    "    # saving feature engineering and selection artifacts within pipeline\n",
    "    #-------------------------------------------------------\n",
    "    obj_list = [fe_pipeline_, fs_pipeline_]\n",
    "    key_list = list(pipe_dict.keys())\n",
    "    for i in range(len(obj_list)):\n",
    "        file_name = obj_list[i].path + '.joblib'\n",
    "        with open(file_name, 'wb') as file:\n",
    "            joblib.dump(pipe_dict[key_list[i]], file)\n",
    "    \n",
    "    # reading model artifact\n",
    "    #-------------------------------------------------------\n",
    "    model_file = 'model.bst'\n",
    "    art_file=f'{path_ref}/{model_file}'\n",
    "    blob = bucket.blob(art_file)\n",
    "    blob.download_to_filename(model_file)\n",
    "    bst = xgb.Booster()\n",
    "    bst.load_model(model_file)\n",
    "\n",
    "    # saving endpoint_information artifact within pipeline\n",
    "    #-------------------------------------------------------\n",
    "    model_.metadata['framework'] = 'xgb'\n",
    "    bst.save_model(model_.path + '.bst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfecf906-beaa-4195-a33e-984197f1d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/gpa-poc-001/churn-base-image-src-xgb@sha256:61db16ec13bba7d8023fff61329c6c28a7eb119f8f837fce4c09258776c16727\",\n",
    "    output_component_file=\"make_predictions.yaml\"\n",
    ")\n",
    "\n",
    "def make_predictions(\n",
    "    Xpred_: Input[Dataset],\n",
    "    cod_cliente_: Input[Dataset],\n",
    "    df_date_: Input[Dataset],\n",
    "    model_: Input[Model],\n",
    "    fe_pipeline_: Input[Model],\n",
    "    fs_pipeline_: Input[Model],\n",
    "    predictions_df_: Output[Dataset],\n",
    "    bucket:str='gpa-churn',\n",
    "    output_path:str='data/processed/batch_output/'\n",
    "    ):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import pytz\n",
    "    import uuid\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from google.cloud import aiplatform\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    sys.path.append('/usr/app/')\n",
    "    sys.path.append('/usr/app/src')\n",
    "    import src.utils as utils\n",
    "    import src.pipeline_modules as pipeline_modules\n",
    "    \n",
    "    # loading artifacts\n",
    "    #-------------------------------------------------------\n",
    "    bst = xgb.Booster()\n",
    "    bst.load_model(model_.path+'.bst')\n",
    "    fe_pipeline = joblib.load(fe_pipeline_.path+'.joblib')\n",
    "    fs_pipeline = joblib.load(fs_pipeline_.path+'.joblib')\n",
    "    \n",
    "    # reading input arguments\n",
    "    #-------------------------------------------------------\n",
    "    Xpred = pd.read_parquet(Xpred_.path+'.parquet')\n",
    "    cod_cliente = pd.read_parquet(cod_cliente_.path+'.parquet')\n",
    "    df_date = pd.read_parquet(df_date_.path+'.parquet')\n",
    "    \n",
    "    # applying pipelines\n",
    "    #-------------------------------------------------------\n",
    "    Xpred = fe_pipeline.transform(Xpred)\n",
    "    Xpred = fs_pipeline.transform(Xpred)\n",
    "    \n",
    "    # making predictions\n",
    "    #-------------------------------------------------------\n",
    "    ypred = list(bst.predict(xgb.DMatrix(Xpred)))\n",
    "    \n",
    "    # applying cod_cliente as index\n",
    "    #-------------------------------------------------------\n",
    "    predictions_df = pd.DataFrame()\n",
    "    predictions_df['cod_cliente'] = cod_cliente['cod_cliente']\n",
    "    predictions_df['churn_prediction'] = ypred\n",
    "    predictions_df['reference_date'] = df_date['date']\n",
    "    predictions_df['prediction_time'] = datetime.now().strftime(format='%Y-%m-%d %H:%M:%S')\n",
    "    predictions_df['model_stage'] = 'poc'\n",
    "    \n",
    "    # saving predictions dataframe in pipeline and in output_path\n",
    "    #-------------------------------------------------------\n",
    "    predictions_df.to_parquet(predictions_df_.path+'.parquet', index=False, compression='gzip')\n",
    "    \n",
    "    # upload predictions output to cloud storage\n",
    "    #-------------------------------------------------------\n",
    "    predictions_df.to_parquet('predictions.parquet', index=False, compression='gzip')\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket)\n",
    "    storage_file= f'{output_path}predictions.parquet'\n",
    "    blob = bucket.blob(storage_file)\n",
    "    blob.upload_from_filename('predictions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f22fc-90a6-418b-9c24-dfc8e0faff2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4070a0-e504-4246-8467-b60591c95ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the pipeline\n",
    "from datetime import datetime\n",
    "timestamp=datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "pipeline_label = f'pipeline-churn-batchprediction-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6b93de-8664-4a23-a12a-1d25c1b92671",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=pipeline_label)\n",
    "\n",
    "def pipeline(\n",
    "    bucket:str='gpa-churn',\n",
    "    input_data_path:str='data/processed/input/',\n",
    "    artifacts_path:str='artifacts/training_pipeline/production/',\n",
    "    predictions_path:str='data/processed/batch_output/'\n",
    "    ):\n",
    "    \n",
    "    data_op = get_prediction_data(\n",
    "        path=f'gs://{bucket}/{input_data_path}'\n",
    "        )\n",
    "    \n",
    "    load_artifacts_op = load_artifacts(\n",
    "        path=f'gs://{bucket}/{artifacts_path}'\n",
    "        )\n",
    "    \n",
    "    make_predictions(\n",
    "        data_op.outputs['Xpred_'],\n",
    "        data_op.outputs['cod_cliente_'],\n",
    "        data_op.outputs['df_date_'],\n",
    "        load_artifacts_op.outputs['model_'],\n",
    "        load_artifacts_op.outputs['fe_pipeline_'],\n",
    "        load_artifacts_op.outputs['fs_pipeline_'],\n",
    "        bucket=bucket,\n",
    "        output_path=predictions_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91545e9d-ca9a-4fe1-be79-f7f717dff4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='ml_pipeline_batchprediction.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a47ca6-608a-488e-80a3-30a94dba0b41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fab0af9f-45d8-4376-9ac0-a3dbbee75940",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=pipeline_label,\n",
    "    template_path=\"ml_pipeline_batchprediction.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "150ab38b-1ea0-4c91-bb5a-e75d314053c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/southamerica-east1/pipelines/runs/pipeline-churn-batchprediction-20220607232504?project=437364709834\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/437364709834/locations/southamerica-east1/pipelineJobs/pipeline-churn-batchprediction-20220607232504\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
